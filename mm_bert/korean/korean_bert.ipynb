{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['Liz', 'ìµœê³ ì£¼ê°€ë“¤ë§Œ íŒŒíŠ¸ ë˜ ë§ì´ ë„£ì—ˆë„¤ã…‹ã…‹ã…‹ã…‹ë¦¬ì¦ˆ, ë ˆì´ëŠ” ê·¸ëƒ¥ ìµœê³ ì£¼ê°€ê°€ ìŠ¤ìŠ¤ë¡œ ë˜ëŠ” ìˆ˜ ë°–ì— ì—†ìœ¼ë ¤ë‚˜ã…‹ã…‹ã…‹ã…‹ã…‹ìŠ¤íƒ€ì‰½ ì§•í•˜ë„¤ ì§„ì§œã…‹ã…‹ã…‹ã…‹ã…‹', 'Liz deserves more lines..!! why is starship unfair to Liz??!!', 'STARSHÄ°P!!! GÄ°VE MORE LÄ°NE FOR OUR LÄ°Z!!!!', 'what does  it mean?', \"Can someone inform me what a 'kitsch' is?ğŸ˜‚\", 'IVEæ„›ã—ã¦ã‚‹\\U0001faf6', 'ãƒªã‚ºè¦šé†’ã—ã¦ã¦ã™ãï¼ï¼', 'What a great song', 'Full Album on April 10th!!!!!', \"A music video literally never angered me more. I love these girls and it's no fault of them, but what the label is doing to this group is so shameful. Poor Liz got no screen time or lines in the song. I repeatedly saw the same girls over and over in this video while she was hidden. They couldn't try any harder to hide her, It's horrible. Someone pick this girl up under a new group so she can be allowed to shine! I'm so tired of certain girls in this group being so praised by the label while Liz is treated like a background character. It's one thing for fandoms to have favorites, but for a label to and make it so obvious is messed up. It's not even like she's bad, her voice is wonderful, she's beautiful inside and out, and her performance skills are great. She knows how to do her job! To be treated like a wall because you don't fit your labels beauty standard is disgusting. They should drop her from the label if they don't like her instead of suppressing her so she can go elsewhere. The song is great, really cool video, and the girls look and sound beautiful, but it's not fair that one of their members is treated so poorly every freaking song/performance.\", '41.038', 'ã‚ã£ã¡ã‚ƒè‰¯ã„^^', 'so unfair, Liz deserves moreğŸ˜”', 'ë¦¬ì¦ˆ íŒŒíŠ¸ ë¬´ìŠ¨ ì¼ì´ì•¼... ì¥ë‚œí•˜ì§€ ë§ê³ ;;', 'ë¦¬ì¦ˆëŠ” ì™œ í•œë²ˆë§Œ ë¶ˆëŸ¬ìš”?', 'ì›ì˜ì´ ë³´ì»¬ì´ ë“£ê¸° í¸ì•ˆí•œ ì†Œë¦¬êµ¬ë‚˜!\\në¹„ìŒë¹¼ë‹ˆê¹Œ ë„ˆë¬´ ì¢‹ë‹¤. ì´ì„œë„ ë¹„ìŒì„ ì¢€ ë¹¼ì•¼ë ë“¯', 'ì²˜ìŒì—”  ë­ì§€  í–ˆëŠ”ë°\\nê³„ì† ë“¤ìœ¼ë‹ˆê¹Œ ì¢‹ì•„ì§€ëŠ” ë…¸ë˜', 'Rei slayed her parts', '41,015,111 M congrats IVE!!!', 'Keren', 'Very nice song', 'å¥½ã', 'holy bang bang bangâ¤', 'They just dropped the concept photos!', 'Ni YouTube bener\" ya ğŸ˜¢ udh cape\" streaming malah di apus mulu view nya ğŸ’”', 'starship distribute lines based on popularity, liz got blonde hair and opening lines as soon as she debuted meaning that starship wanted to popularize her, but obviously she took that for granted', 'I much prefer this group to New Jeans. IVE does music right.', 'yt donÂ´t delete ive views??', 'Ã©pica', 'ame esta canciÃ³n', \"Amazing as always\\nI'm so into this song! Let's go 45M\", 'ì´ì„œ ë„ˆë¬´ ì˜ˆì˜ë‹¤', 'OOTD BITHC OOTD!!!!!!!', 'Nos estan eliminando vistas ,por eso no avanzamos \\n      P*TO  YOUTUBE (con todas las ganas de ofender , pero a la ves no XD)', 'visuaaaalll love', 'ë‚˜ë§Œ ë®¤ë¹„ êµ¬ë¦¬ê²Œ ëŠê»´ì§€ë‚˜? ëŸ¬ë¸Œ ë‹¤ì´ë¸Œ ê°™ì€ ê±° ë³´ê³  ì‹¶ì€ë°', 'ë§¤ì¼ ë“£ê³  ìˆì–´ìš”\\në…¸ë˜ ë„ˆë¬´ ì¢‹ë‹¤', \"We can't deny wonyoung's visual is a wow factor\", '0:07 ãƒãƒ‹ãƒ†ã‚¦ã‚©ãƒ‹ãƒ§ãƒ³ãƒ“ã‚¸ãƒ¥è‰¯ã™ã', 'They knew Liz would slay if she had more lines', 'ì˜ ë³´ê³  ë“¤ì—ˆìŠµë‹ˆë‹¤.', 'ã†ã‚ãƒ¼ã¾ãŸæ¥ãŸã‚ˆç¥æ›²', 'yknow what this sounds lie? soty', 'wake up in the morning', 'I dunno but this song such a downgrade for me... Eleven and Love Dive still in my heart, After Like is 50:50', 'Starship do it so good I know what do u do like that for lizğŸ˜‚', \"788k like >40m>ads<flop ğŸ˜…ğŸ˜…ğŸ˜…they're only popular in Korean digital music\", 'Liz got me', 'Ya estaba en 41M youtube...ğŸ˜¡', 'EstÃ¡n eliminando vistas :/', \"40M again!! YouTube! Stop it's 41M but YouTube keeps deleting it\", \"The music is quite addicted and the scene is stunning too,however,the transition of the mv seems a little bit orderless......It's kind of confusion.\", 'lets go dive!', 'felt bad for liz', 'The chorus', 'Is Liz leaving Ive?', 'love this song!!!ğŸ˜ğŸ˜ğŸ˜', 'Epic track.... all IVE tracks are epic', 'Liz â¤', 'woonyoung !!!! <3', 'ä»Šå¹´ã‚‚IVEã®å¹´ã«ã™ã‚‹ããƒ¼ï¼', 'ä»Šå›ã®mvã‚‚æœ€é«˜', 'ya me da miedo que todas las canciones de ive sean buenas', 'The fact that youtube kept deleting our views... we could have easily passed 45 mil by now ğŸ™„', '41.023', 'REI ROSITA NUNCA T VAYAS', 'go', 'Finalmente este grupito demostrando su pobre popularidad con este MV llamado chish que sacaron.\\nUna vergueza de numeros.\\nFracaso.', 'ì¥ì›ì˜ ê°œì´ì˜ë‹¤!!!!!!!!!!!â¤â¤â¤', 'ì´ ë…¸ë˜ ë¦¬ì¦ˆ ìŒìƒ‰ ë“£ê³ ,ê¹œì§ ë†€ë¬ì—ˆëŠ”ë°,ì œ ë”¸ì´ í•œíŒŒíŠ¸ë¼ê³  í•˜ë”êµ°ìš”.\\nê·¸ ì–˜ê¸°ë“£ê³ ,ë„ˆë¬´ ë†€ë¬ì–´ìš”.\\ní•œíŒŒíŠ¸ë¼ë„  ì²˜ìŒë“¤ì—ˆë˜ ì œ ê·€ì—” ì œì¼ ê½‚í˜”ë„¤ìš”~', 'ê·¸ë¦¬ê³  ë§¤í¬ë¡œ ë°”ì´ëŸ´ ëŒ“ê¸€ì´ë¼ ì§€ë“¤ ìƒˆë¼ ì•„ì´ë””ë§Œ ë‹µê¸€ìœ¼ë£¨ë‹¬ìˆ˜ ìˆê²Œ ì¡°ì‘í•´ë†“ìŒ ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ ì£„ë‹¤ ì˜ì–´ì„ ì•„ì´ë¸Œê°€ ê·¸ë ‡ê²Œ í•´ì™¸ì—ì„œ ì¸ê¸°ê°€ ìˆì—ˆë‹¤ê³ ', 'go dive!', 'Can we have a 100 hours loop of Liz part please', 'ì´ê±°ë´ë¼ ì¿ í‚¤ ë‹¬ê³  ì•„ì´ë¸Œ í™ë³´í•˜ëŸ¬ ë‹¤ë‹ˆëŠ”ê±°....ì´ëŸ°ê²Œ ë°”ë¡œ ë°”ì´ëŸ´ ì£¼ì‘ì´ì§€ ì•„ì´ë¸Œ=ì£¼ì‘ê·¸ë£¹', '21th PERFECT ALL KILL\\nKitsch â€“ Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=) #1 Vibe (=) #1 Youtube (=) #1 Spotify (=) #1 Apple (=) ğŸ”¥ğŸ¤­', '41 M !!! LETS GO 50 M ğŸ¦', \"PAK! Well deserved. I just can't stop myself from watching this 100x times a day lol\", 'ë…¸ë˜ëŒ€ë°•ì´ë‹¤â¤â¤â¤â¤â¤', '20th PERFECT ALL KILL\\nKitsch â€“ Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\nğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸', \"_From highly-anticipated returns to can't-miss debut releases, check out 15 albums dropping this April from Linkin Park, IVE, Rae Sremmurd, and more_\\n\\n- Grammy Award News\", 'ë…¸ë˜ ì¤‘ë…ì„±ìˆë‹¤', 'NewJeans win IVEğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ’šğŸ’šğŸ’šğŸ’šğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ˜¡ğŸ¤£ğŸ¤£ğŸ˜¡ğŸ˜¡ğŸ¤£ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜˜ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡', 'Kakao/Melon/IVE ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰every time IVE can be No.1ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰', '2:53 is my favorite part in this mv', 'did liz nasty', 'NewJeans win IVEğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜˜ğŸ˜˜ğŸ˜˜ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡ğŸ˜¡', 'ì¡°íšŒìˆ˜ 4ì²œ1ë°±ë§Œ ëˆˆì•...!', 'ë®¤ë¹„ìƒ‰ê°ì´ë‘ ì˜ìƒ ì§„ì§œ ì˜ ë½‘ì•˜ë‹¤\\nìŠ¤íƒ€ì‰½ì´ ë°°ìš´ ë³€íƒœ', 'ë©”ì¸ íƒ€ì´í‹€ì—ëŠ” ë¦¬ì¦ˆ íŒŒíŠ¸ê°€ ë§ê¸¸......ğŸ˜®\\u200dğŸ’¨', 'ë©”ë³´ê°€ ë©”ë³´ê°€ ì•„ë‹Œ ê·¸ë£¹', \"20th PERFECT ALL KILL\\n 'Kitsch' â€“ 10AM KST: \\n\\n#1 MelOn (=) \\n#1 FLO (=) \\n#1 Genie (=) \\n#1 Bugs (=) \\n\\nMelOn ULs: 398,081 (+1,163)\", 'malarda', 'ë¦¬ì¦ˆ íŒŒíŠ¸ê°€ ë„ˆë¬´ ì—†ì–´ìš”ã… ã…œ', '20th PERFECT ALL KILL\\nKitsch â€“ Update : #1 MelOn (=) #1 FLO (=) #1 Genie (+1) #1 Bugs (=)\\nğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸ğŸ˜â˜ï¸', \"Liz's part is so short but sweet , looking forward to the full new album. I hope everyone has their proper line distribution.\", 'YUH YUH YUH YUH YUH YUH YUH', 'It looks like Liz is not part of the group anymore.', 'I LOVE REI', 'THEIR CONFIDENCE >>>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ë¦¬ë·°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ìµœê³ ì£¼ê°€ë“¤ë§Œ íŒŒíŠ¸ ë˜ ë§ì´ ë„£ì—ˆë„¤ã…‹ã…‹ã…‹ã…‹ë¦¬ì¦ˆ, ë ˆì´ëŠ” ê·¸ëƒ¥ ìµœê³ ì£¼ê°€ê°€ ìŠ¤ìŠ¤ë¡œ ë˜ëŠ” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liz deserves more lines..!! why is starship un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STARSHÄ°P!!! GÄ°VE MORE LÄ°NE FOR OUR LÄ°Z!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what does  it mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Liz's part is so short but sweet , looking for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>YUH YUH YUH YUH YUH YUH YUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>It looks like Liz is not part of the group any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I LOVE REI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>THEIR CONFIDENCE &gt;&gt;&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ë¦¬ë·°\n",
       "0                                                 Liz\n",
       "1   ìµœê³ ì£¼ê°€ë“¤ë§Œ íŒŒíŠ¸ ë˜ ë§ì´ ë„£ì—ˆë„¤ã…‹ã…‹ã…‹ã…‹ë¦¬ì¦ˆ, ë ˆì´ëŠ” ê·¸ëƒ¥ ìµœê³ ì£¼ê°€ê°€ ìŠ¤ìŠ¤ë¡œ ë˜ëŠ” ...\n",
       "2   Liz deserves more lines..!! why is starship un...\n",
       "3          STARSHÄ°P!!! GÄ°VE MORE LÄ°NE FOR OUR LÄ°Z!!!!\n",
       "4                                 what does  it mean?\n",
       "..                                                ...\n",
       "95  Liz's part is so short but sweet , looking for...\n",
       "96                        YUH YUH YUH YUH YUH YUH YUH\n",
       "97  It looks like Liz is not part of the group any...\n",
       "98                                         I LOVE REI\n",
       "99                               THEIR CONFIDENCE >>>\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(my_list)\n",
    "my_df.rename(columns={0:'ë¦¬ë·°'}, inplace=True)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì‚¬ë‘</td>\n",
       "      <td>ë‚˜ë„ ì•Œì•„ ë‚  ê´´ë¡­íˆëŠ” ê²ƒë„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì‚¬ë‘</td>\n",
       "      <td>ê³ ë¯¼ì€ í•­ìƒ ë‚  ë¬´ë„ˆíŠ¸ë¦¬ë„¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì‚¬ë‘</td>\n",
       "      <td>ì„¸ìƒì— ë§˜ê» ë‚  ìŸì•„ë¶€ì–´ë„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì‚¬ë‘</td>\n",
       "      <td>ë‚¨ì•„ìˆëŠ” ê±´ ë‚˜ í•˜ë‚˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì‚¬ë‘</td>\n",
       "      <td>ì´ì   ë‚˜ë¥¼ ë³´ë©´ ì°¡í•´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>ë¶„ë…¸</td>\n",
       "      <td>ì–´ê¹¨ ì¡ê³  í”ë“¤ê³ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6346</th>\n",
       "      <td>ë¶„ë…¸</td>\n",
       "      <td>ë¯¸ì¹œ ë“¯ì´ ìš•í•˜ê³ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>ë¶„ë…¸</td>\n",
       "      <td>ì°”ëŸ¬ ìƒì²˜ ë‚´ë´ë„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>ë¶„ë…¸</td>\n",
       "      <td>ì‚¬ë‘í–ˆì–´ ì •ë§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>ë¶„ë…¸</td>\n",
       "      <td>ì•„ë‹ˆ Fuck you ë„ˆë‚˜ êº¼ì ¸</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6350 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion              lyric\n",
       "0         ì‚¬ë‘    ë‚˜ë„ ì•Œì•„ ë‚  ê´´ë¡­íˆëŠ” ê²ƒë„\n",
       "1         ì‚¬ë‘     ê³ ë¯¼ì€ í•­ìƒ ë‚  ë¬´ë„ˆíŠ¸ë¦¬ë„¤\n",
       "2         ì‚¬ë‘     ì„¸ìƒì— ë§˜ê» ë‚  ìŸì•„ë¶€ì–´ë„\n",
       "3         ì‚¬ë‘        ë‚¨ì•„ìˆëŠ” ê±´ ë‚˜ í•˜ë‚˜\n",
       "4         ì‚¬ë‘        ì´ì   ë‚˜ë¥¼ ë³´ë©´ ì°¡í•´\n",
       "...      ...                ...\n",
       "6345      ë¶„ë…¸          ì–´ê¹¨ ì¡ê³  í”ë“¤ê³ \n",
       "6346      ë¶„ë…¸          ë¯¸ì¹œ ë“¯ì´ ìš•í•˜ê³ \n",
       "6347      ë¶„ë…¸          ì°”ëŸ¬ ìƒì²˜ ë‚´ë´ë„\n",
       "6348      ë¶„ë…¸            ì‚¬ë‘í–ˆì–´ ì •ë§\n",
       "6349      ë¶„ë…¸  ì•„ë‹ˆ Fuck you ë„ˆë‚˜ êº¼ì ¸\n",
       "\n",
       "[6350 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°ì •ìˆëŠ” ê°€ì‚¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ë³´ì\n",
    "\n",
    "emo_data = pd.read_csv('/home/cshoon036/MixMind/mm_bert/korean/emo_data.csv')\n",
    "emo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ì‚¬ë‘', 'í–‰ë³µ', 'ì‹ ë‚˜ëŠ”', 'ì—´ì •', 'ìŠ¬í””', 'ê·¸ë¦¬ì›€', 'ì™¸ë¡œì›€', 'ë‘ë ¤ì›€', 'ë¶„ë…¸'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_data['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(emotion):\n",
    "    if emotion == 0:\n",
    "        return None\n",
    "    # ê¸ì •ì ì¸ ê°ì •ì€ 1ë¡œ ì¹˜í™˜\n",
    "    elif (emotion == \"ì‚¬ë‘\" or emotion == \"í–‰ë³µ\" or emotion == \"ì‹ ë‚˜ëŠ”\" or emotion == \"ì—´ì •\"):\n",
    "        return 1\n",
    "    # ë¶€ì •ì ì¸ ê°ì •ì€ 0ìœ¼ë¡œ ì¹˜í™˜\n",
    "    elif (emotion == \"ìŠ¬í””\" or emotion == \"ê·¸ë¦¬ì›€\" or emotion == \"ì™¸ë¡œì›€\" or emotion == \"ë‘ë ¤ì›€\", emotion == \"ë¶„ë…¸\"):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸/ë¶€ì • ì¹˜í™˜\n",
    "emo_data['sentiment'] = emo_data['emotion'].map(lambda x : get_sentiment(x))\n",
    "\n",
    "emo_data = emo_data[(emo_data['sentiment'] == 1) | (emo_data['sentiment'] == 0)]\n",
    "emo_data['sentiment'] = emo_data['sentiment'].astype('int64')\n",
    "emo_data = emo_data[['lyric','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë‚˜ë„ ì•Œì•„ ë‚  ê´´ë¡­íˆëŠ” ê²ƒë„</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê³ ë¯¼ì€ í•­ìƒ ë‚  ë¬´ë„ˆíŠ¸ë¦¬ë„¤</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì„¸ìƒì— ë§˜ê» ë‚  ìŸì•„ë¶€ì–´ë„</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë‚¨ì•„ìˆëŠ” ê±´ ë‚˜ í•˜ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì´ì   ë‚˜ë¥¼ ë³´ë©´ ì°¡í•´</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6345</th>\n",
       "      <td>ì–´ê¹¨ ì¡ê³  í”ë“¤ê³ </td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6346</th>\n",
       "      <td>ë¯¸ì¹œ ë“¯ì´ ìš•í•˜ê³ </td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>ì°”ëŸ¬ ìƒì²˜ ë‚´ë´ë„</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348</th>\n",
       "      <td>ì‚¬ë‘í–ˆì–´ ì •ë§</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6349</th>\n",
       "      <td>ì•„ë‹ˆ Fuck you ë„ˆë‚˜ êº¼ì ¸</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6350 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lyric  sentiment\n",
       "0       ë‚˜ë„ ì•Œì•„ ë‚  ê´´ë¡­íˆëŠ” ê²ƒë„          1\n",
       "1        ê³ ë¯¼ì€ í•­ìƒ ë‚  ë¬´ë„ˆíŠ¸ë¦¬ë„¤          1\n",
       "2        ì„¸ìƒì— ë§˜ê» ë‚  ìŸì•„ë¶€ì–´ë„          1\n",
       "3           ë‚¨ì•„ìˆëŠ” ê±´ ë‚˜ í•˜ë‚˜          1\n",
       "4           ì´ì   ë‚˜ë¥¼ ë³´ë©´ ì°¡í•´          1\n",
       "...                 ...        ...\n",
       "6345          ì–´ê¹¨ ì¡ê³  í”ë“¤ê³           0\n",
       "6346          ë¯¸ì¹œ ë“¯ì´ ìš•í•˜ê³           0\n",
       "6347          ì°”ëŸ¬ ìƒì²˜ ë‚´ë´ë„          0\n",
       "6348            ì‚¬ë‘í–ˆì–´ ì •ë§          0\n",
       "6349  ì•„ë‹ˆ Fuck you ë„ˆë‚˜ êº¼ì ¸          0\n",
       "\n",
       "[6350 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 08:56:53.628768: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "loading file vocab.txt from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "  0%|          | 0/6350 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6350/6350 [00:00<00:00, 6377.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents: 6350, # labels: 6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from emotion_analysis import bert_tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sent, train_label in tqdm(zip(emo_data[\"lyric\"], emo_data[\"sentiment\"]), total=len(emo_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n",
    "\n",
    "train_lyric_input_ids = np.array(input_ids, dtype=int)\n",
    "train_lyric_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_lyric_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_lyric_inputs = (train_lyric_input_ids, train_lyric_attention_masks, train_lyric_type_ids)\n",
    "\n",
    "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #ë ˆì´ë¸” í† í¬ë‚˜ì´ì§• ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "print(\"# sents: {}, # labels: {}\".format(len(train_lyric_input_ids), len(train_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed ê³ ì •\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 25\n",
    "VALID_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  8888 36553 10892  9959 14871  8985  9294 70162 15184 12692 77884\n",
      "   102     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[CLS] ê³ ë¯¼ì€ í•­ìƒ ë‚  ë¬´ë„ˆíŠ¸ë¦¬ë„¤ [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_id = train_lyric_input_ids[1]\n",
    "attention_mask = train_lyric_attention_masks[1]\n",
    "token_type_id = train_lyric_type_ids[1]\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 08:57:10.678780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.704111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.704483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.705285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 08:57:10.705750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.706001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:10.706188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-05 08:57:11.377996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14618 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\n",
      "loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file tf_model.h5 from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tf_model.h5\n",
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from emotion_analysis import TFBertClassifier\n",
    "\n",
    "sentiment_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/tensorflow_addons/optimizers/rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "# ì´ batch size * 4 epoch = 2344 * 4\n",
    "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*2, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "sentiment_model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tf2_bert_sentiment -- Folder already exists \n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mixmind/lib/python3.8/site-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.6331\n",
      "Epoch 1: val_loss improved from inf to 0.91802, saving model to ./tf2_bert_sentiment/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n",
      "2023-04-05 09:01:00.546163: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at save_restore_v2_ops.cc:138 : RESOURCE_EXHAUSTED: tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__SaveV2_dtypes_618_device_/job:localhost/replica:0/task:0/device:CPU:0}} tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m cp_callback \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     21\u001b[0m     checkpoint_path, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m ,save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m , save_weight_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# í•™ìŠµê³¼ eval ì‹œì‘\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m history \u001b[39m=\u001b[39m sentiment_model\u001b[39m.\u001b[39;49mfit(train_lyric_inputs, train_data_labels, epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     25\u001b[0m                     validation_split \u001b[39m=\u001b[39;49m VALID_SPLIT, callbacks\u001b[39m=\u001b[39;49m[earlystop_callback, cp_callback])\n\u001b[1;32m     27\u001b[0m \u001b[39m#steps_for_epoch\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(history\u001b[39m.\u001b[39mhistory)\n",
      "File \u001b[0;32m/opt/conda/envs/mixmind/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/envs/mixmind/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__SaveV2_dtypes_618_device_/job:localhost/replica:0/task:0/device:CPU:0}} tf2_bert_sentiment/best_model/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate1257553640276108254; No space left on device [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"tf2_bert_sentiment\"\n",
    "\n",
    "# overfittingì„ ë§‰ê¸° ìœ„í•œ ealrystop ì¶”ê°€\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=3)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1ë²ˆ ì´ìƒ ìƒìŠ¹ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ)\\\n",
    "\n",
    "checkpoint_path = os.path.join(\"./\", model_name, 'best_model')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss', verbose=1, mode='min' ,save_best_only=True , save_weight_only=True)\n",
    "\n",
    "# í•™ìŠµê³¼ eval ì‹œì‘\n",
    "history = sentiment_model.fit(train_lyric_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "\n",
    "#steps_for_epoch\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in my_df.itertuples():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emotion_analysis import sentence_convert_data, sentence_evaluation_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
